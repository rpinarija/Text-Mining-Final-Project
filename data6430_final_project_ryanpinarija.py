# -*- coding: utf-8 -*-
"""DATA6430_Final_Project_ryanpinarija.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fndk0G8Z5gk3XgSkz9UeyIemViuE--KC
"""

!pip install requests
!pip install asyncio
!pip install pycountry
!pip install wikipedia
!pip install geopy
!pip install contractions
!pip install spacy
!pip install pyLDAvis

# Commented out IPython magic to ensure Python compatibility.
import requests
import asyncio
import pycountry
import wikipedia
from geopy.distance import geodesic

from sentence_transformers import SentenceTransformer

import contractions                                                                                             # contractions dictionary
from string import punctuation

import spacy

import nltk, re, pprint
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('gutenberg')

import pandas as pd                                                     # import pandas
import numpy as np                                                      # import numpy
import matplotlib.pyplot as plt                                         # import matplotlib
# %matplotlib inline

import sklearn
from sklearn.metrics import classification_report
from sklearn.linear_model import SGDClassifier, LogisticRegression
from sklearn.model_selection import train_test_split
import pyLDAvis
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer
from sklearn.metrics.pairwise import cosine_similarity                  # import cosine_similarity from sklearn.metrics.pairwise
from sklearn.decomposition import LatentDirichletAllocation, PCA             # import LatentDirichlectAllocation from sklearn.decomposition
from scipy.cluster.hierarchy import fcluster, dendrogram, linkage
from sklearn.cluster import KMeans

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.text import text_to_word_sequence
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing.sequence import pad_sequences

from string import punctuation
from urllib import request
from bs4 import BeautifulSoup

import gensim
from gensim.corpora import Dictionary
from gensim.models.coherencemodel import CoherenceModel
from gensim.models import Word2Vec, KeyedVectors
from gensim.scripts.glove2word2vec import glove2word2vec


import warnings
warnings.filterwarnings("ignore")

"""# Method 1 - Compiling Corpora via APIs, Preprocessing & Normalization"""

with open('/content/data6420_final_project_api_key.txt', 'r') as file:
  api_key = file.readline().strip()

page_length = 5  # number of objects per page
offset = 0  # offset from first object in the list
count = 0  # total objects count
lon = None  # place longitude
lat = None  # place latitude
min_description_length = 50

async def api_get(method, query=None):
    try:
        otm_api = f"https://api.opentripmap.com/0.1/en/places/{method}?apikey={api_key}"
        if query:
            otm_api += f"&{query}"
        response = requests.get(otm_api)
        return response.json()
    except requests.exceptions.RequestException as err:
        print(f"Fetch Error: {err}")
        return None

async def search_place(places):
    results = []
    for name in places:
        print(f"\nSearching for: {name}")
        data = await api_get("geoname", f"name={name}")

        if data and data.get('status') == "OK":
            lon = data['lon']
            lat = data['lat']
            country = get_country_name(data['country'])
            print(f"Location found: {name}, {country}")
            count, places_info = await first_load(lon, lat, country, name)  # Pass city name to preserve
            for place in places_info:
                results.append(place)
        else:
            print(f"Name not found for: {name}")
            results.append({"cities": name, "error": "Name not found"})
    return results

async def first_load(lon, lat, country, city_name):
    global count, offset
    radius = 4000  # Start with a radius of 1000 meters
    data = await api_get(
        "radius",
        f"radius={radius}&limit={page_length}&offset={offset}&lon={lon}&lat={lat}&rate=2&format=json"
    )

    places_info = []

    if data:
        count = len(data)
        for place in data:
            detailed_info = await get_place_details(place['xid'], country, place['dist'], place['kinds'], city_name)
            places_info.append(detailed_info)
        offset = 0
        return count, places_info
    return 0, []

# Function to calculate the distance between two coordinate points
def are_coordinates_too_close(coord1, coord2, threshold=10):
    return geodesic(coord1, coord2).meters < threshold

async def get_place_details(xid, country, distance, kinds, city_name, prev_coordinates=None):
    # Fetch detailed info about a place using its xid
    data = await api_get(f"xid/{xid}")

    if data:
        description = data.get("wikipedia_extracts", {}).get("text") or data.get("info", {}).get("descr", "")
        new_coordinates = (data.get("point", {}).get("lat"), data.get("point", {}).get("lon"))

        # Check if coordinates are too close to previous coordinates (avoid redundant re-fetch)
        if prev_coordinates and are_coordinates_too_close(prev_coordinates, new_coordinates):
            print(f"Skipping place near identical coordinates for {city_name} (xid: {xid}).")
            return None  # Skip this place

        # Check if description is too short or empty and retry fetching
        if len(description) < min_description_length:
            print(f"Insufficient description for {city_name} (xid: {xid}). Re-fetching...")

            # Attempt to re-fetch data from the same xid
            re_fetched_data = await api_get(f"xid/{xid}")

            # Re-fetch only if it's a different place with more useful info
            if re_fetched_data and re_fetched_data != data:
                description = re_fetched_data.get("wikipedia_extracts", {}).get("text") or re_fetched_data.get("info", {}).get("descr", "No description available")
            else:
                # If re-fetch yields no useful information, try Wikipedia as a fallback
                description = fetch_wikipedia_description(city_name)

        return {
            "xid": xid,
            "cities": city_name,  # Now, the actual city name is stored in the "cities" column
            "countries": country,  # Full country name is used here
            "lon": data.get("point", {}).get("lon"),
            "lat": data.get("point", {}).get("lat"),
            "distance": distance,  # Add distance from the center
            "kinds": kinds.split(","),  # Convert the list of kinds to a Python list
            "place_type": data.get("name", "Unknown"),  # This stores the name/type of the place, such as "museum" or "landmark"
            "rating": data.get("rate", "No rating"),  # Rating range from 1-3 where the additional 'h' refers to a place with cultural heritage
            "wikidata_id": data.get("wikidata", "N/A"),  # Get wikidata ID if available
            "description": description  # Wiki description of the location
        }
    return {"xid": xid, "error": "Details not found"}

# Fallback to Wikipedia if OpenTripMap doesn't provide a sufficient description
def fetch_wikipedia_description(city_name):
    try:
        return wikipedia.summary(city_name, sentences=2)  # Fetch a brief summary from Wikipedia
    except wikipedia.exceptions.PageError:
        return "No description available"

# Function to convert country code to full country name using pycountry
def get_country_name(country_code):
    try:
        country = pycountry.countries.get(alpha_2=country_code)
        return country.name if country else "Unknown Country"
    except KeyError:
        return "Unknown Country"

places_list = ["Amsterdam", "Bangkok", "Barcelona", "Berlin", "Brasilia", "Cape Town", "Chicago", "Dubai", "Ho Chi Minh City", "Jakarta", "Los Angeles", "London", "Melbourne", "New York",
               "Paris", "The Maldives", "Vienna"]

# Start the search process
search_data = await search_place(places_list)

df = pd.DataFrame(search_data)

search_data = pd.read_csv("/content/data6420_final_project_data.csv")

search_data.drop_duplicates(subset=['description'], keep='first', inplace=True)
search_data.reset_index(drop=True, inplace=True)

df = pd.DataFrame(search_data)
df.rename(columns={"name": "city"}, inplace=True)

lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english') +
                 stopwords.words('dutch') +
                 stopwords.words('indonesian') +
                 stopwords.words('german') +
                 stopwords.words('spanish') +
                 stopwords.words('portuguese') +
                 stopwords.words('french') +
                 stopwords.words('nepali') +
                 stopwords.words('arabic'))

def preprocess_text(doc):

    # Remove html
    doc = re.sub(r'<[^>]+>', '', doc)
    # Remove non-alphanumeric characters
    doc = re.sub(r'[^a-zA-Z0-9\s]', '', doc, re.I|re.A)

    # Convert text to lowercase
    doc = doc.lower()
    # Remove extra whitespace
    doc = doc.strip()

    # Tokenize and remove stopwords, then Lemmatize
    tokens = [lemmatizer.lemmatize(word) for word in word_tokenize(doc) if word not in stop_words]
    return ' '.join(tokens)

df['clean_text'] = df['description'].apply(preprocess_text)

"""# Method 2 - Bag of Words Models"""

tv = TfidfVectorizer(
    # Remove terms that appear in fewer than 3 documents and over 95% of documents
    min_df=3,
    max_df=0.95,
    ngram_range=(2, 2),
)

dtm = tv.fit_transform(df['clean_text'])
vocabulary = np.array(tv.get_feature_names_out())                                             # convert to array)

dtm.shape

pd.DataFrame(dtm.toarray(), columns=vocabulary, index=['Doc_'+str(i) for i in range(len(df['clean_text']))]).head()

"""# Method 3 - Topic Modeling"""

lda = LatentDirichletAllocation(n_components = 10, random_state=42)
lda.fit(dtm)

def display_topics(model, feature_names, no_top_words):
    for topic_idx, topic in enumerate(model.components_):
        print("Topic %d:" % (topic_idx))
        print(", ".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))


no_top_words = 10
display_topics(lda, vocabulary, no_top_words)

def custom_tokenizer(text):
    tokens = nltk.word_tokenize(text)
    # Create unigrams
    tokens = [token for token in tokens if token not in stopwords.words('english')]
    # Create bigrams
    bigrams = ["_".join(tokens[i:i+2]) for i in range(len(tokens)-1)]
    return bigrams

tokenized_texts = [custom_tokenizer(text) for text in df['clean_text']] # create a list of bigram strings for each paper
print(tokenized_texts)

gensim_dict = Dictionary(tokenized_texts)
gensim_dict.filter_extremes(no_below=3, no_above=0.85) # we need to set this to the same filtering we did during our vectorization step
print(gensim_dict)

# Container to hold coherence values
coherence_values = []

# Set the range of topics to examine
topic_nums = range(10,25)


for num_topics in topic_nums:
    # Fit LBA model for each value of topic_nums
    lda_model = LatentDirichletAllocation(n_components=num_topics, random_state=42)
    lda_model.fit(dtm)

    # Extract top features for each topic, replace with vocab word
    word_ids = lda_model.components_.argsort(axis=1)[:, ::-1][:, :10]
    topics = [[tv.get_feature_names_out()[i].replace(" ", "_") for i in topic_word_ids] for topic_word_ids in word_ids]

    # Print extracted topics
    print(f"Extracted topics for num_topics={num_topics}:")
    for idx, topic in enumerate(topics):
        print(f"Topic {idx}: {topic}")

    # Calculate and append coherence scores
    coherence_model_lda = CoherenceModel(topics=topics, texts=tokenized_texts, dictionary=gensim_dict, coherence='u_mass')
    coherence_lda = coherence_model_lda.get_coherence()
    print(f"UMass for num_topics={num_topics}: {coherence_lda}\n")

    coherence_values.append(coherence_lda)

# Plotting
plt.figure(figsize=(10, 6))
plt.plot(topic_nums, coherence_values, marker='o')
plt.title('UMass Score vs. Number of Topics')
plt.xlabel('Number of Topics')
plt.ylabel('UMass Score')
plt.grid(True)
plt.show()

# Optimal topic number
lda = LatentDirichletAllocation(n_components=18, random_state=42)
doc_topic = lda.fit(dtm)

no_top_words = 17
display_topics(doc_topic, vocabulary, no_top_words)

doc_topic_matrix = lda.transform(dtm)

df_doc_topic = pd.DataFrame(doc_topic_matrix, columns=[f'Topic {i}' for i in range(lda.n_components)])
df_doc_topic

# Vectorize documents
vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')
doc_term_matrix = vectorizer.fit_transform(df['clean_text'])

# Fit LDA model
lda_model = LatentDirichletAllocation(n_components=5, random_state=42)
lda_model.fit(doc_term_matrix)

# Document-topic distribution
doc_topic_dist = lda_model.transform(doc_term_matrix)

# Prepare inputs for pyLDAvis
doc_lengths = doc_term_matrix.sum(axis=1).A1  # Sum word counts for each document
term_frequency = doc_term_matrix.sum(axis=0).A1  # Sum word counts for each term
vocab = vectorizer.get_feature_names_out()

# Visualize
lda_visualization = pyLDAvis.prepare(
    topic_term_dists=lda_model.components_,
    doc_topic_dists=doc_topic_dist,
    doc_lengths=doc_lengths,
    vocab=vocab,
    term_frequency=term_frequency
)
pyLDAvis.display(lda_visualization)

import pyLDAvis
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.feature_extraction.text import CountVectorizer

def train_lda_model(df, num_topics=5):
    vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')
    doc_term_matrix = vectorizer.fit_transform(df['clean_text'])

    lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)
    lda.fit(doc_term_matrix)

    return lda, vectorizer, doc_term_matrix

lda_model, vectorizer, doc_term_matrix = train_lda_model(df)

# Visualize topics
import pyLDAvis
pyLDAvis.enable_notebook()

# Calculate doc_lengths, term_frequency, and vocab manually
doc_lengths = doc_term_matrix.sum(axis=1).A1
term_frequency = doc_term_matrix.sum(axis=0).A1
vocab = vectorizer.get_feature_names_out()

# Get the document-topic distribution
doc_topic_dist = lda_model.transform(doc_term_matrix)

# Pass all required arguments to pyLDAvis.prepare(), including doc_topic_dist
lda_visualization = pyLDAvis.prepare(lda_model.components_,
                                      doc_topic_dist, # Pass doc_topic_dist here
                                      doc_lengths=doc_lengths,
                                      vocab=vocab,
                                      term_frequency=term_frequency)

"""# Method 4 - Word Embeddings"""

stop_words = set(stopwords.words('english') +
                 stopwords.words('dutch') +
                 stopwords.words('indonesian') +
                 stopwords.words('german') +
                 stopwords.words('spanish') +
                 stopwords.words('portuguese') +
                 stopwords.words('french') +
                 stopwords.words('nepali') +
                 stopwords.words('arabic'))

# Define the text cleaning function
def clean_text(text):
    text = text.lower()
    words = word_tokenize(text)

    # Stopwords removal
    # Use the global stop_words set that you created earlier
    words = [word for word in words if word not in stop_words]

    # Lemmatization
    lemmatizer = WordNetLemmatizer()
    words = [lemmatizer.lemmatize(word) for word in words]

    return words

tokens = [clean_text(doc) for doc in df['description']]
tokens

!unzip /content/drive/MyDrive/glove.6B.zip

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Input Training Word2Vec model parameters
# model_w2v = Word2Vec(sentences=tokens, vector_size=50, window=5, min_count=1, workers=4)
# 
# # If we have the GloVe model downloaded and we'll convert Glove vectors in text format into the word2vec text format:
# glove_input_file = '/content/glove.6B.50d.txt'
# 
# word2vec_output_file = 'glove.6B.50d.word2vec'
# glove2word2vec(glove_input_file, word2vec_output_file)
# 
# # load the converted model
# model_glove = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False) # input parameters

def average_word_vectors(words, model, vocabulary, num_features):
    feature_vector = np.zeros((num_features,),dtype="float64")
    nwords = 0.

    for word in words:
        if word in vocabulary:
            nwords = nwords + 1.
            feature_vector = np.add(feature_vector, model[word])

    if nwords:
        feature_vector = np.divide(feature_vector, nwords)

    return feature_vector

def averaged_word_vectorizer(corpus, model, num_features):
    if isinstance(model, Word2Vec):
        vocabulary = set(model.wv.index_to_key)
        features = [average_word_vectors(tokenized_sentence, model.wv, vocabulary, num_features)
                        for tokenized_sentence in corpus]
    else:  # this part is for GloVe model, which is loaded as KeyedVectors
        vocabulary = set(model.index_to_key)
        features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)
                        for tokenized_sentence in corpus]

    return np.array(features)

glove_feature_array = averaged_word_vectorizer(corpus=tokens, model=model_glove, num_features=50)

glove_feature_array.shape

glove_feature_array

vectorizer = TfidfVectorizer()
X_tfidf = vectorizer.fit_transform(df['clean_text'])

X_tfidf.shape

# Perform KMeans clustering
kmeans_glove = KMeans(n_clusters=3, random_state=42).fit(glove_feature_array)
kmeans_tfidf = KMeans(n_clusters=3, random_state=42).fit(X_tfidf)

# Function to plot PCA
def plot_pca(data, labels, model_name):
    pca = PCA(n_components=2)
    scatter_plot_points = pca.fit_transform(data)

    colors = ["red", "blue", "green"]

    plt.scatter(scatter_plot_points[:, 0], scatter_plot_points[:, 1], c=[colors[i] for i in labels])

    # add labels
    for i, label in enumerate(labels):
        plt.text(scatter_plot_points[i, 0], scatter_plot_points[i, 1], str(i))

    plt.title('PCA plot for '+ model_name)
    plt.show()

# Plotting PCA plots
plot_pca(glove_feature_array, kmeans_glove.labels_, 'GloVe')
plot_pca(X_tfidf.toarray(), kmeans_tfidf.labels_, 'TF-IDF')

"""# Streamlit App"""

# !pip install streamlit

import streamlit as st

# Load Data and pre-trained model
df = pd.read_csv("/content/data6420_final_project_data.csv")
model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')

# Process description and create embeddings
if 'embeddings' not in df.columns:
    df['clean_text'] = df['description'].apply(preprocess_text)
    df['embeddings'] = df['clean_text'].apply(lambda x: model.encode(x))


# Similarity Function: Use cosine similarity to compare user input and locations.
def get_recommendations(user_input, df, top_n=5):
  # Encode user input
    user_embedding = model.encode(user_input)

    # Calculate cosine similarity scores
    similarities = cosine_similarity([user_embedding], np.stack(df['embeddings']))[0]

    # Sort location by similarity
    df['similarity'] = similarities
    recommendations = df.sort_values(by='similarity', ascending=False).head(top_n)

    # Return top N locations
    return recommendations[['location', 'similarity']]

def find_similar_locations(location, df, top_n=5):
  user_embedding = model.encode(location)
  similarities = cosine_similarity([user_embedding], np.stack(df['embeddings']))[0]
  df['similarity'] = similarities
  return df.sort_values(by='similarity', ascending=False).head(top_n)

# Streamlit Title
st.title("Vacation Recommendation App")

st.sidebar.header("User Input")
mode = st.sidebar.radio("Choose a feature:", ["Recommend Vations", "Compare Locations", "Explore Locations"])

# Main Page
if mode == "Recommend Vacations":
    st.header("Vacation Recommendations")
    user_input = st.text_input("Describe your dream vacation (e.g. beaches and sunsets)")
    if user_input:
        recommendations = get_recommendations(user_input, df)
        st.write("Top Recommendations:")
        st.table(recommendations[['location', 'similarity']])
elif mode == "Compare Locations":
    st.header("Compare Similar Locations")
    location = st.selectbox("Choose a location to compare:", df['location'].unique())
    if location:
        similar_locations = find_similar_locations(location, df)
        st.write(f"Similar Locations to {location}:")
        st.table(similar_locations[['location', 'similarity']])

elif mode == "Explore Locations":
    st.header("Explore Locations")
    location = st.selectbox("Choose a location to explore:", df['location'].unique())
    if location:
        st.write(f"Description of {location}:")
        st.write(df.loc[df['location'] == location, 'description'].iloc[0])

!streamlit run app.py

!pip install streamlit

"""### Topic Modeling"""

stop_words = set(stopwords.words('english') +
                 stopwords.words('dutch') +
                 stopwords.words('indonesian') +
                 stopwords.words('german') +
                 stopwords.words('spanish') +
                 stopwords.words('portuguese') +
                 stopwords.words('french') +
                 stopwords.words('nepali') +
                 stopwords.words('arabic'))

def train_lda_model (df, num_topics=5):
  vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words=stop_words)
  dtm = vectorizer.fit_transform(df['clean_text'])

  lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)
  lda.fit(dtm)

  return lda, vectorizer

lda_model, vectorizer = train_lda_model(df)

import pyLDAvis
pyLDAvis.enable_notebook()
lda_visualization = pyLDAvis.sklearn.prepare(lda_model, dtm, vectorizer)

